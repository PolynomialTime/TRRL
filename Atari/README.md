# Atari Environment - TRRL

This project provides training and evaluation code for Atari environments using TRRL (Trust Region Reinforcement Learning) algorithms.

---

## ğŸ“¦ Project Structure

```
Atari/
â”œâ”€â”€ expert_data/           # Pre-collected expert trajectories and datasets
â”œâ”€â”€ AIRL.py                # Adversarial Inverse Reinforcement Learning (AIRL) implementation
â”œâ”€â”€ BC.py                  # Behavioral Cloning (BC) implementation
â”œâ”€â”€ GAIL.py                # Generative Adversarial Imitation Learning (GAIL)
â”œâ”€â”€ SQIL.py                # Soft Q Imitation Learning (SQIL)
â”œâ”€â”€ firl.py                # Feature-based Inverse Reinforcement Learning (FIRL)
â”œâ”€â”€ reward_function.py     # Reward function definitions
â”œâ”€â”€ rollouts.py            # Environment rollouts
â”œâ”€â”€ trrl.py                # TRRL algorithm core
â”œâ”€â”€ arguments.py           # Argument parser configuration
â””â”€â”€ main.py                # Main training entry point
```

---

## ğŸš€ How to Run Atari Training

You can run the training script by specifying the task (Atari environment) name using `--env_name` argument.

### Example Command:
```bash
python main.py --env_name=PongNoFrameskip-v4
```

This command will:
- Train the model on the **Pong** Atari environment.
- Load expert trajectories from `expert_data/transitions_PongNoFrameskip-v4.npy`.

---

## âœ… Supported Tasks

| Task Name                  | Description          |
|----------------------------|----------------------|
| PongNoFrameskip-v4         | Pong Atari game      |
| SpaceInvadersNoFrameskip-v4| Space Invaders Atari game |
| QbertNoFrameskip-v4        | Q*bert Atari game    |
| BreakoutNoFrameskip-v4     | Breakout Atari game  |
| CartPole-v1                | CartPole (classic control) |
| FrozenLake-v1              | Frozen Lake (classic control) |

---

## âš™ï¸ Command Line Arguments

| Argument         | Description                           | Example                           |
|------------------|---------------------------------------|-----------------------------------|
| `--env_name`     | The environment/task to run           | `--env_name=PongNoFrameskip-v4`   |
| `--algo`         | Algorithm to use (BC, SQIL, AIRL, etc.) | `--algo=SQIL`                    |
| `--num_episodes` | Number of episodes for training       | `--num_episodes=5000`            |

You can combine these flags depending on your needs.

---

## ğŸ’¾ Expert Data

Expert datasets are stored in:
```
Atari/expert_data/
```
Each task has:
- `transitions_<TaskName>.npy` files (state-action trajectories)
- Pre-collected expert rollouts in `.zip` format

---

## ğŸ”§ Dependencies

Install required Python packages:
```bash
pip install -r requirements.txt
```

Or manually:
```bash
pip install gym numpy torch
```

---

## âœ¨ Example Training Runs

### Run Behavioral Cloning (BC) on Pong:
```bash
python BC.py --env_name=PongNoFrameskip-v4
```

### Run SQIL on Space Invaders:
```bash
python SQIL.py --env_name=SpaceInvadersNoFrameskip-v4
```

### Run AIRL on Q*bert:
```bash
python AIRL.py --env_name=QbertNoFrameskip-v4
```

---

## ğŸ“‚ Output and Logs
Training results and logs will be saved automatically under:
```
logs/
```

---

## ğŸ“ Notes
- Make sure the `expert_data` folder contains the correct trajectory files for your selected task.
- For larger models or datasets, consider using **Git LFS**.

---
